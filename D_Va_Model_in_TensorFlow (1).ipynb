{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade pandas-ta scikit-learn numpy\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, metrics\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_ta as ta\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import math\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "O5ghd59wMIkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    # List the exact names of your CSV files here.\n",
        "    DATA_FILES = ['AAPL.csv', 'AMZN.csv', 'MSFT.csv']\n",
        "\n",
        "    # --- KEY CHANGE: Our target is now 'returns' ---\n",
        "    TARGET_COL = 'returns'\n",
        "\n",
        "    # A stable sequence length that is divisible by 4\n",
        "    SEQUENCE_LENGTH = 16\n",
        "\n",
        "    # Stable starting hyperparameters\n",
        "    LATENT_DIM = 64\n",
        "    BATCH_SIZE = 16\n",
        "    EPOCHS = 40\n",
        "    LEARNING_RATE = 1e-4\n",
        "    DIFFUSION_STEPS = 100\n",
        "\n",
        "    # Number of input features for the main forecaster\n",
        "    NUM_FEATURES = 5  # Close, RSI, MACD, MACDs, MACDh"
      ],
      "metadata": {
        "id": "4PrV0TJ3Qd0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(file_list, sequence_length):\n",
        "    \"\"\"\n",
        "    MODIFIED to predict percentage returns instead of absolute price.\n",
        "    This is the professional standard for financial time series.\n",
        "    \"\"\"\n",
        "    all_files = file_list\n",
        "    if not all_files:\n",
        "        raise ValueError(\"The DATA_FILES list in the Config class is empty.\")\n",
        "\n",
        "    for f in all_files:\n",
        "        if not os.path.exists(f):\n",
        "            raise FileNotFoundError(f\"The file '{f}' was not found. Please ensure it has been uploaded.\")\n",
        "\n",
        "    all_x, all_y = [], []\n",
        "\n",
        "    feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    # Returns are small numbers centered around 0. We scale them to be between -1 and 1.\n",
        "    target_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\n",
        "    # --- Create a temporary DataFrame to fit the scalers consistently ---\n",
        "    df_list_for_scaling = []\n",
        "    for f in all_files:\n",
        "\n",
        "        df = pd.read_csv(f) # Fallback if 'Date' column is not there or not parsable\n",
        "        df.columns = df.columns.str.lower()\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        df['returns'] = df['close'].pct_change()\n",
        "        df_list_for_scaling.append(df)\n",
        "\n",
        "    full_df_for_scaling = pd.concat(df_list_for_scaling)\n",
        "    full_df_for_scaling.ta.rsi(close='close', length=14, append=True)\n",
        "    full_df_for_scaling.ta.macd(close='close', fast=12, slow=26, signal=9, append=True)\n",
        "    full_df_for_scaling.dropna(inplace=True)\n",
        "\n",
        "    feature_cols = ['close', 'RSI_14', 'MACD_12_26_9', 'MACDs_12_26_9', 'MACDh_12_26_9']\n",
        "    feature_scaler.fit(full_df_for_scaling[feature_cols])\n",
        "    target_scaler.fit(full_df_for_scaling[['returns']])\n",
        "\n",
        "    # --- Process each file individually to create sequences ---\n",
        "    for f in all_files:\n",
        "\n",
        "        df = pd.read_csv(f)\n",
        "        df.columns = df.columns.str.lower()\n",
        "        df.sort_values(by='date', inplace=True)\n",
        "        df['returns'] = df['close'].pct_change()\n",
        "        df.ta.rsi(close='close', length=14, append=True)\n",
        "        # Use high/low if available for indicators like ATR in the future\n",
        "        # df.ta.atr(high='high', low='low', close='close', length=14, append=True)\n",
        "        df.ta.macd(close='close', fast=12, slow=26, signal=9, append=True)\n",
        "        df.dropna(inplace=True)\n",
        "\n",
        "        if len(df) < 2 * sequence_length:\n",
        "            continue\n",
        "\n",
        "        features_df = df[feature_cols]\n",
        "        target_df = df[['returns']]\n",
        "\n",
        "        scaled_features = feature_scaler.transform(features_df)\n",
        "        scaled_target = target_scaler.transform(target_df)\n",
        "\n",
        "        for i in range(len(scaled_features) - 2 * sequence_length):\n",
        "            all_x.append(scaled_features[i:i + sequence_length])\n",
        "            all_y.append(scaled_target[i + sequence_length:i + 2 * sequence_length])\n",
        "\n",
        "    return np.array(all_x, dtype=np.float32), np.array(all_y, dtype=np.float32), target_scaler"
      ],
      "metadata": {
        "id": "bXMldvLxMprt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    steps = timesteps + 1; x = tf.linspace(0.0, float(timesteps), steps)\n",
        "    alphas_cumprod = tf.cos(((x / float(timesteps)) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]; betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return tf.clip_by_value(betas, 0.0001, 0.9999)\n",
        "\n",
        "def get_diffusion_variables(beta_schedule):\n",
        "    alphas = 1.0 - beta_schedule; alphas_cumprod = tf.math.cumprod(alphas, axis=0)\n",
        "    return {\"betas\": beta_schedule, \"alphas_cumprod\": alphas_cumprod, \"sqrt_alphas_cumprod\": tf.sqrt(alphas_cumprod), \"sqrt_one_minus_alphas_cumprod\": tf.sqrt(1.0 - alphas_cumprod)}\n",
        "\n",
        "def forward_diffusion(x0, t, diff_vars):\n",
        "    noise = tf.random.normal(shape=tf.shape(x0)); sqrt_alphas_cumprod_t = tf.gather(diff_vars[\"sqrt_alphas_cumprod\"], t); sqrt_one_minus_alphas_cumprod_t = tf.gather(diff_vars[\"sqrt_one_minus_alphas_cumprod\"], t)\n",
        "    sqrt_alphas_cumprod_t = tf.reshape(sqrt_alphas_cumprod_t, [-1, 1, 1]); sqrt_one_minus_alphas_cumprod_t = tf.reshape(sqrt_one_minus_alphas_cumprod_t, [-1, 1, 1])\n",
        "    return sqrt_alphas_cumprod_t * x0 + sqrt_one_minus_alphas_cumprod_t * noise, noise"
      ],
      "metadata": {
        "id": "Ju4IJcPgMpuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TensorFlow/Keras D-Va Model Architecture ---\n",
        "\n",
        "class SE(layers.Layer):\n",
        "    def __init__(self, in_channels, r=4, **kwargs):\n",
        "        super().__init__(**kwargs); self.squeeze = layers.GlobalAveragePooling1D(); self.excitation = models.Sequential([layers.Dense(in_channels // r, activation='relu'), layers.Dense(in_channels, activation='sigmoid')])\n",
        "    def call(self, inputs):\n",
        "        x = self.squeeze(inputs); x = self.excitation(x); x = tf.expand_dims(x, axis=1); return inputs * x"
      ],
      "metadata": {
        "id": "OspyE2AMMpwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder Residual Cell\n",
        "class EncoderResidualCell(layers.Layer):\n",
        "    def __init__(self, out_channels, stride=1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.stride = stride\n",
        "        self.conv_layers = None\n",
        "        self.shortcut = None\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        in_channels = input_shape[-1]\n",
        "        self.conv_layers = models.Sequential([\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Activation('swish'),\n",
        "            layers.Conv1D(self.out_channels, 3, padding='same', strides=self.stride),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Activation('swish'),\n",
        "            layers.Conv1D(self.out_channels, 3, padding='same'),\n",
        "            SE(self.out_channels)\n",
        "        ])\n",
        "        if self.stride > 1 or in_channels != self.out_channels:\n",
        "            self.shortcut = layers.Conv1D(self.out_channels, 1, strides=self.stride)\n",
        "        else:\n",
        "            self.shortcut = layers.Lambda(lambda x: x)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.conv_layers(inputs) + self.shortcut(inputs)"
      ],
      "metadata": {
        "id": "b4AQoNW3Mpy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder Residual Cell\n",
        "class DecoderResidualCell(layers.Layer):\n",
        "    def __init__(self, out_channels, scale_factor=2, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.scale_factor = scale_factor\n",
        "        self.out_channels = out_channels\n",
        "        self.upsample = layers.UpSampling1D(size=self.scale_factor)\n",
        "        self.conv_layers = None\n",
        "        self.shortcut = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        in_channels = input_shape[-1]\n",
        "        self.conv_layers = models.Sequential([\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Activation('swish'),\n",
        "            layers.Conv1D(self.out_channels, 3, padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Activation('swish'),\n",
        "            layers.Conv1D(self.out_channels, 3, padding='same'),\n",
        "            SE(self.out_channels)\n",
        "        ])\n",
        "        self.shortcut = layers.Conv1D(self.out_channels, 1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x_up = self.upsample(inputs)\n",
        "        return self.conv_layers(x_up) + self.shortcut(x_up)"
      ],
      "metadata": {
        "id": "4x6RpvCTMp1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling layer for the VAE reparameterization trick\n",
        "class Sampling(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.random.normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "metadata": {
        "id": "15-0Pl0PMp3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_nvae(sequence_length, latent_dim, num_features):\n",
        "    encoder_input = layers.Input(shape=(sequence_length, num_features))\n",
        "    x = EncoderResidualCell(32)(encoder_input)\n",
        "    x = EncoderResidualCell(64, stride=2)(x)\n",
        "    x = EncoderResidualCell(128, stride=2)(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "    z = Sampling()([z_mean, z_log_var])\n",
        "    encoder = models.Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "\n",
        "    latent_input = layers.Input(shape=(latent_dim,))\n",
        "    x = layers.Dense(128)(latent_input)\n",
        "    x = layers.Reshape((1, 128))(x)\n",
        "    x = DecoderResidualCell(64, scale_factor=4)(x)\n",
        "    x = DecoderResidualCell(32, scale_factor=2)(x)\n",
        "    x = DecoderResidualCell(16, scale_factor=2)(x)\n",
        "    x = layers.Conv1D(1, 1)(x)\n",
        "    decoder_output = x\n",
        "    decoder = models.Model(latent_input, decoder_output, name=\"decoder\")\n",
        "    return encoder, decoder"
      ],
      "metadata": {
        "id": "j1kBbFWsMp5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The complete D-Va Model with custom training logic\n",
        "class DVaModel(models.Model):\n",
        "    def __init__(self, sequence_length, latent_dim, num_features, diff_vars, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_features = num_features\n",
        "        self.diff_vars = diff_vars\n",
        "        self.nvae_encoder, self.nvae_decoder = build_nvae(sequence_length, latent_dim, num_features)\n",
        "        self.denoise_encoder, self.denoise_decoder = build_nvae(sequence_length, latent_dim, 1)\n",
        "        self.total_loss_tracker = metrics.Mean(name=\"total_loss\")\n",
        "        self.mse_loss_tracker = metrics.Mean(name=\"mse_loss\")\n",
        "        self.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\n",
        "        self.dsm_loss_tracker = metrics.Mean(name=\"dsm_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.total_loss_tracker, self.mse_loss_tracker, self.kl_loss_tracker, self.dsm_loss_tracker]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x_batch, y_batch = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            t = tf.random.uniform([tf.shape(x_batch)[0]], minval=0, maxval=len(self.diff_vars['betas']), dtype=tf.int32)\n",
        "            xt, noise_x = forward_diffusion(x_batch, t, self.diff_vars)\n",
        "            yt, noise_y = forward_diffusion(y_batch, t, self.diff_vars)\n",
        "            z_mean, z_log_var, z = self.nvae_encoder(xt, training=True)\n",
        "            y_recon = self.nvae_decoder(z, training=True)\n",
        "            y_recon_reshaped = tf.reshape(y_recon, [-1, self.sequence_length, 1])\n",
        "            _, _, z_denoise = self.denoise_encoder(y_recon_reshaped, training=True)\n",
        "            denoise_grad_pred = self.denoise_decoder(z_denoise, training=True)\n",
        "            mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "            loss_mse = mse_loss_fn(yt, y_recon)\n",
        "            loss_kl = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1))\n",
        "            loss_dsm = mse_loss_fn(noise_y, denoise_grad_pred)\n",
        "            total_loss = loss_mse + 0.1 * loss_kl + 0.5 * loss_dsm\n",
        "\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.mse_loss_tracker.update_state(loss_mse)\n",
        "        self.kl_loss_tracker.update_state(loss_kl)\n",
        "        self.dsm_loss_tracker.update_state(loss_dsm)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def predict_step(self, x):\n",
        "        y_pred_noisy = self.nvae_decoder(self.nvae_encoder(x, training=False)[2], training=False)\n",
        "        noise_est = self.denoise_decoder(self.denoise_encoder(y_pred_noisy, training=False)[2], training=False)\n",
        "        return y_pred_noisy - noise_est"
      ],
      "metadata": {
        "id": "cvW5S9HjMp8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Training and Evaluation Script (CORRECTED) ---\n",
        "if __name__ == '__main__':\n",
        "    cfg = Config()\n",
        "    tf.random.set_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # --- Part 1: Training ---\n",
        "    try:\n",
        "        x, y, scaler = load_and_preprocess_data(cfg.DATA_FILES, cfg.SEQUENCE_LENGTH)\n",
        "        x_train, _, y_train, _ = train_test_split(x, y, test_size=0.2, shuffle=False)\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(1024).batch(cfg.BATCH_SIZE)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during data loading: {e}\")\n",
        "        exit()\n",
        "\n",
        "    betas = cosine_beta_schedule(timesteps=cfg.DIFFUSION_STEPS)\n",
        "    diff_vars = get_diffusion_variables(betas)\n",
        "    dva_model = DVaModel(cfg.SEQUENCE_LENGTH, cfg.LATENT_DIM, cfg.NUM_FEATURES, diff_vars)\n",
        "    dva_model.compile(optimizer=optimizers.AdamW(learning_rate=cfg.LEARNING_RATE))\n",
        "\n",
        "    print(\"--- Starting Training (Predicting Returns) ---\")\n",
        "    dva_model.fit(train_dataset, epochs=cfg.EPOCHS)\n",
        "\n",
        "    # --- Part 2: Evaluation ---\n",
        "    print(\"\\n--- Evaluating on Test Set for Each Stock Separately ---\")\n",
        "\n",
        "    for stock_file in cfg.DATA_FILES:\n",
        "        try:\n",
        "            stock_name = stock_file.split('.')[0]\n",
        "            print(f\"\\n--- Results for {stock_name} ---\")\n",
        "\n",
        "            single_stock_list = [stock_file]\n",
        "            x_stock, y_stock, stock_scaler = load_and_preprocess_data(single_stock_list, cfg.SEQUENCE_LENGTH)\n",
        "\n",
        "            if len(x_stock) == 0:\n",
        "                print(f\"Skipping {stock_file}: Not enough data.\")\n",
        "                continue\n",
        "\n",
        "            _, x_test_stock, _, y_test_stock = train_test_split(x_stock, y_stock, test_size=0.2, shuffle=False)\n",
        "\n",
        "            if len(x_test_stock) == 0:\n",
        "                print(f\"Skipping {stock_file}: No test data available after split.\")\n",
        "                continue\n",
        "\n",
        "            predicted_y_scaled = dva_model.predict_step(x_test_stock).numpy()\n",
        "            actual_y_scaled = y_test_stock\n",
        "\n",
        "            predicted_returns = stock_scaler.inverse_transform(predicted_y_scaled.reshape(-1, cfg.SEQUENCE_LENGTH))\n",
        "            actual_returns = stock_scaler.inverse_transform(actual_y_scaled.reshape(-1, cfg.SEQUENCE_LENGTH))\n",
        "\n",
        "            predicted_next_day_return = predicted_returns[:, 0]\n",
        "            actual_next_day_return = actual_returns[:, 0]\n",
        "\n",
        "            # --- Plotting ---\n",
        "            plt.style.use('seaborn-v0_8-darkgrid')\n",
        "            plt.figure(figsize=(18, 9))\n",
        "            plt.plot(actual_next_day_return, color='blue', label='Actual Next-Day Return', alpha=0.7)\n",
        "            plt.plot(predicted_next_day_return, color='red', label='Predicted Next-Day Return', linestyle='--', alpha=0.7)\n",
        "            plt.title(f'D-Va Model: Predicted vs. Actual Returns for {stock_name}', fontsize=18)\n",
        "            plt.xlabel(f'Time (Test Set for {stock_name})', fontsize=14)\n",
        "            plt.ylabel('Percentage Return', fontsize=14)\n",
        "            plt.axhline(0, color='grey', linestyle='--')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "            # --- Metrics ---\n",
        "            mae = np.mean(np.abs(predicted_next_day_return - actual_next_day_return))\n",
        "            rmse = np.sqrt(np.mean((predicted_next_day_return - actual_next_day_return)**2))\n",
        "            print(f\"Metrics for {stock_name}:\")\n",
        "            print(f\"  Mean Absolute Error (MAE): {mae:.6f}\")\n",
        "            print(f\"  Root Mean Squared Error (RMSE): {rmse:.6f}\")\n",
        "\n",
        "            trading_signals = np.sign(predicted_next_day_return)\n",
        "            strategy_returns = trading_signals * actual_next_day_return\n",
        "\n",
        "            if np.std(strategy_returns) > 0:\n",
        "                sharpe_ratio = (np.mean(strategy_returns) / np.std(strategy_returns)) * np.sqrt(252)\n",
        "            else:\n",
        "                sharpe_ratio = 0.0\n",
        "            print(f\"  Annualized Sharpe Ratio: {sharpe_ratio:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nCould not process or plot for {stock_file}. Error: {e}\")\n"
      ],
      "metadata": {
        "id": "NaUEuXqhMqB1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "309ce314-45a3-420a-e484-805715c1efce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during data loading: 'Cannot get left slice bound for non-unique label: np.int64(25)'\n",
            "--- Starting Training (Predicting Returns) ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-321118871.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- Starting Training (Predicting Returns) ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdva_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# --- Part 2: Evaluation ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DLDh2JyGAoHr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}